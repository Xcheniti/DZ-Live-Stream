#!/usr/bin/env python3
"""
ğŸ”¥ ECHOROUK SUPER EXTRACTOR V4.0 - THE ULTIMATE STREAM EXTRACTOR
ğŸ¯ Features: Multi-method extraction, Automatic validation, Smart caching
âš¡ Optimized for GitHub Actions with zero dependencies issues
"""

import os
import sys
import re
import json
import time
import hashlib
import requests
import subprocess
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Tuple
from urllib.parse import urlparse, urljoin, quote

# =============== CONFIGURATION ===============
CONFIG = {
    'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'timeout': 20,
    'max_retries': 3,
    'cache_duration': 3600,  # 1 hour
    'target_urls': [
        "https://www.echoroukonline.com/live-news",
        "https://www.echoroukonline.com/tv",
        "https://www.echoroukonline.com/ar/tv/live",
        "https://www.echoroukonline.com/fr/tv-en-direct"
    ],
    'cdn_patterns': [
        r'echorouk.*\.m3u8',
        r'v7\.vcloud.*\.m3u8',
        r'algeriatv.*\.m3u8',
        r'dzcdn.*\.m3u8',
        r'stream\.alaan.*\.m3u8'
    ],
    'backup_streams': [
        "https://shls-echorouk-news.v7.vcloud.dz/echorouk_news/index.m3u8",
        "https://live.alaan.tv/echorouk/live.m3u8",
        "https://cdn.algeriatv.dz/live/echorouk.m3u8",
        "https://stream.dztv.dz/echorouk/live.m3u8"
    ]
}

# =============== LOGGER ===============
class Logger:
    @staticmethod
    def info(msg):
        print(f"[{datetime.now().strftime('%H:%M:%S')}] â„¹ï¸ {msg}")
    
    @staticmethod
    def success(msg):
        print(f"[{datetime.now().strftime('%H:%M:%S')}] âœ… {msg}")
    
    @staticmethod
    def warning(msg):
        print(f"[{datetime.now().strftime('%H:%M:%S')}] âš ï¸ {msg}")
    
    @staticmethod
    def error(msg):
        print(f"[{datetime.now().strftime('%H:%M:%S')}] âŒ {msg}")

# =============== CACHE MANAGER ===============
class CacheManager:
    def __init__(self):
        self.cache_dir = "cache"
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def get_cache_key(self, url):
        return hashlib.md5(url.encode()).hexdigest()[:16]
    
    def get(self, url):
        try:
            key = self.get_cache_key(url)
            cache_file = os.path.join(self.cache_dir, f"{key}.json")
            
            if os.path.exists(cache_file):
                with open(cache_file, 'r') as f:
                    data = json.load(f)
                
                cache_time = datetime.fromisoformat(data['timestamp'])
                if datetime.now() - cache_time < timedelta(seconds=CONFIG['cache_duration']):
                    return data['content']
        except:
            pass
        return None
    
    def set(self, url, content):
        try:
            key = self.get_cache_key(url)
            cache_file = os.path.join(self.cache_dir, f"{key}.json")
            
            data = {
                'timestamp': datetime.now().isoformat(),
                'url': url,
                'content': content
            }
            
            with open(cache_file, 'w') as f:
                json.dump(data, f)
        except:
            pass

# =============== SMART EXTRACTOR ===============
class EchoroukSuperExtractor:
    def __init__(self):
        self.session = requests.Session()
        self.cache = CacheManager()
        self.setup_session()
        self.results = {
            'start_time': datetime.now().isoformat(),
            'methods_tried': [],
            'urls_found': [],
            'urls_working': [],
            'best_url': None,
            'final_m3u': None
        }
    
    def setup_session(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ù„Ø³Ø© HTTP Ù…ØªÙ‚Ø¯Ù…Ø©"""
        self.session.headers.update({
            'User-Agent': CONFIG['user_agent'],
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.9,en;q=0.8,fr;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Referer': 'https://www.echoroukonline.com/',
            'DNT': '1'
        })
    
    # =============== METHOD 1: YT-DLP (MOST POWERFUL) ===============
    def extract_with_ytdlp(self, url):
        """Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£Ù‚ÙˆÙ‰ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… yt-dlp"""
        Logger.info(f"Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 1: Ø§Ø³ØªØ®Ø¯Ø§Ù… yt-dlp Ù„Ù€ {url}")
        
        try:
            cmd = [
                'yt-dlp',
                '--no-warnings',
                '--quiet',
                '--skip-download',
                '--dump-json',
                '--user-agent', CONFIG['user_agent'],
                '--referer', 'https://www.echoroukonline.com/',
                url
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                data = json.loads(result.stdout)
                
                # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚Ø§Øª
                urls_found = []
                
                # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ formats
                if 'formats' in data:
                    for fmt in data['formats']:
                        if 'url' in fmt and '.m3u8' in fmt['url']:
                            urls_found.append(fmt['url'])
                
                # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ requested_formats
                if 'requested_formats' in data:
                    for fmt in data['requested_formats']:
                        if 'url' in fmt and '.m3u8' in fmt['url']:
                            urls_found.append(fmt['url'])
                
                # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ url Ù…Ø¨Ø§Ø´Ø±Ø©
                if 'url' in data and '.m3u8' in data['url']:
                    urls_found.append(data['url'])
                
                if urls_found:
                    Logger.success(f"yt-dlp ÙˆØ¬Ø¯ {len(urls_found)} Ø±ÙˆØ§Ø¨Ø·")
                    self.results['methods_tried'].append('ytdlp_success')
                    return list(set(urls_found))
        
        except Exception as e:
            Logger.warning(f"yt-dlp ÙØ´Ù„: {str(e)[:50]}")
        
        self.results['methods_tried'].append('ytdlp_failed')
        return []
    
    # =============== METHOD 2: DEEP HTML PARSING ===============
    def extract_from_html(self, url):
        """ØªØ­Ù„ÙŠÙ„ HTML Ø¨Ø¹Ù…Ù‚ Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Ù…Ø®ÙÙŠØ©"""
        Logger.info(f"Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 2: ØªØ­Ù„ÙŠÙ„ HTML Ù„Ù€ {url}")
        
        cached = self.cache.get(url)
        if cached:
            html = cached
            Logger.info("Ø§Ø³ØªØ®Ø¯Ø§Ù… HTML Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©")
        else:
            try:
                response = self.session.get(url, timeout=CONFIG['timeout'])
                response.raise_for_status()
                html = response.text
                self.cache.set(url, html)
            except Exception as e:
                Logger.warning(f"ÙØ´Ù„ Ø¬Ù„Ø¨ HTML: {e}")
                self.results['methods_tried'].append('html_fetch_failed')
                return []
        
        # Ø£Ù†Ù…Ø§Ø· Ø¨Ø­Ø« Ø´Ø§Ù…Ù„Ø©
        patterns = [
            # Ø£Ù†Ù…Ø§Ø· JavaScript Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø©
            r'(?:src|file|url)\s*[=:]\s*["\'](https?://[^"\']+?\.m3u8(?:\?[^"\']+)?)["\']',
            
            # Ø£Ù†Ù…Ø§Ø· JSON
            r'["\'](?:playlist|sources|stream)["\']\s*:\s*\[?\s*{?[^}]*["\'](?:src|file|url)["\']\s*:\s*["\'](https?://[^"\']+?\.m3u8)["\']',
            
            # Ø£Ù†Ù…Ø§Ø· HTML5 video
            r'<video[^>]+data-setup=[\'"][^\'"]*["\']file["\']\s*:\s*["\'](https?://[^"\']+?\.m3u8)["\']',
            
            # Ø±ÙˆØ§Ø¨Ø· CDN Ø®Ø§ØµØ©
            r'(https?://[^"\'\s<>]+/(?:live|stream|hls)/[^"\'\s<>]+\.m3u8)',
            
            # Ø±ÙˆØ§Ø¨Ø· Ø¹Ø§Ù…Ø© ØªÙ†ØªÙ‡ÙŠ Ø¨Ù€ m3u8
            r'(https?://[^"\'\s<>]+\.m3u8(?:\?[^"\'\s<>]*)?)',
        ]
        
        found_urls = []
        for pattern in patterns:
            try:
                matches = re.findall(pattern, html, re.IGNORECASE)
                for match in matches:
                    clean_url = self.clean_url(match)
                    if clean_url and clean_url not in found_urls:
                        found_urls.append(clean_url)
            except:
                continue
        
        if found_urls:
            Logger.success(f"HTML parsing ÙˆØ¬Ø¯ {len(found_urls)} Ø±ÙˆØ§Ø¨Ø·")
            self.results['methods_tried'].append('html_parse_success')
        else:
            self.results['methods_tried'].append('html_parse_failed')
        
        return found_urls
    
    # =============== METHOD 3: SMART CDN DISCOVERY ===============
    def discover_cdn_urls(self):
        """Ø§ÙƒØªØ´Ø§Ù Ø±ÙˆØ§Ø¨Ø· CDN Ø°ÙƒÙŠØ©"""
        Logger.info("Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 3: Ø§ÙƒØªØ´Ø§Ù Ø±ÙˆØ§Ø¨Ø· CDN Ø°ÙƒÙŠØ©")
        
        cdn_urls = []
        
        # ØªÙˆÙ„ÙŠØ¯ Ø±ÙˆØ§Ø¨Ø· CDN Ù…Ø­ØªÙ…Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
        base_domains = [
            "https://cdn.echoroukonline.com",
            "https://stream.echoroukonline.com",
            "https://live.echoroukonline.com",
            "https://tv.echorouk.tv",
            "https://v7.vcloud.dz",
            "https://cdn.algeriatv.dz"
        ]
        
        paths = [
            "/live/stream.m3u8",
            "/hls/stream.m3u8",
            "/echorouk/live.m3u8",
            "/echorouk_news/index.m3u8",
            "/live/echorouk.m3u8",
            "/stream/playlist.m3u8"
        ]
        
        for domain in base_domains:
            for path in paths:
                test_url = domain + path
                cdn_urls.append(test_url)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        cdn_urls.extend(CONFIG['backup_streams'])
        
        Logger.info(f"ØªÙ… ØªÙˆÙ„ÙŠØ¯ {len(cdn_urls)} Ø±Ø§Ø¨Ø· CDN Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±")
        self.results['methods_tried'].append('cdn_discovery')
        
        return cdn_urls
    
    # =============== METHOD 4: NETWORK REQUEST ANALYSIS ===============
    def analyze_network_requests(self, url):
        """Ù…Ø­Ø§ÙƒØ§Ø© Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ© Ù„Ø§ÙƒØªØ´Ø§Ù Ø±ÙˆØ§Ø¨Ø· Ø®ÙÙŠØ©"""
        Logger.info(f"Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 4: ØªØ­Ù„ÙŠÙ„ Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ© Ù„Ù€ {url}")
        
        api_endpoints = [
            f"{url}/config.json",
            f"{url}/manifest.m3u8",
            f"{url}/playlist.m3u8",
            url.replace("live-news", "api/stream"),
            url.replace("live-news", "api/v1/live"),
            "https://www.echoroukonline.com/api/stream/live",
            "https://www.echoroukonline.com/json/live.json"
        ]
        
        found_urls = []
        for api_url in api_endpoints:
            try:
                response = self.session.get(
                    api_url,
                    timeout=10,
                    headers={'X-Requested-With': 'XMLHttpRequest'}
                )
                
                if response.status_code == 200:
                    content = response.text
                    
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
                    m3u8_matches = re.findall(
                        r'(https?://[^\s"\']+\.m3u8[^\s"\']*)',
                        content
                    )
                    
                    for match in m3u8_matches:
                        clean_url = self.clean_url(match)
                        if clean_url:
                            found_urls.append(clean_url)
                            
            except:
                continue
        
        if found_urls:
            Logger.success(f"ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø¨ÙƒØ© ÙˆØ¬Ø¯ {len(found_urls)} Ø±ÙˆØ§Ø¨Ø·")
            self.results['methods_tried'].append('network_analysis_success')
        else:
            self.results['methods_tried'].append('network_analysis_failed')
        
        return found_urls
    
    # =============== URL VALIDATION ===============
    def validate_stream_url(self, url):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø´Ø§Ù…Ù„ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø±Ø§Ø¨Ø·"""
        if not url or not isinstance(url, str):
            return False
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø·
        clean_url = self.clean_url(url)
        if not clean_url:
            return False
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        if not clean_url.startswith('http'):
            return False
        
        if '.m3u8' not in clean_url.lower():
            return False
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø±Ø§Ø¨Ø·
        try:
            # Ø·Ù„Ø¨ HEAD Ø£ÙˆÙ„Ø§Ù‹ (Ø£Ø³Ø±Ø¹)
            head_response = self.session.head(
                clean_url,
                timeout=10,
                allow_redirects=True,
                headers={'Range': 'bytes=0-0'}
            )
            
            if head_response.status_code in [200, 206, 302, 307]:
                # Ø¥Ø°Ø§ Ù†Ø¬Ø­ HEADØŒ Ø§Ø®ØªØ¨Ø± Ø¬Ø²Ø¡ ØµØºÙŠØ± Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                response = self.session.get(
                    clean_url,
                    timeout=10,
                    stream=True,
                    headers={'Range': 'bytes=0-500'}
                )
                
                if response.status_code in [200, 206]:
                    content = response.text[:500]
                    
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù‡Ùˆ m3u8 Ø­Ù‚ÙŠÙ‚ÙŠ
                    is_valid = any([
                        '#EXTM3U' in content,
                        '#EXTINF' in content,
                        '.ts' in content,
                        '.m3u8' in content.lower()
                    ])
                    
                    if is_valid:
                        return clean_url
        
        except Exception as e:
            Logger.warning(f"ÙØ´Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† {clean_url[:50]}: {str(e)[:30]}")
        
        return False
    
    # =============== URL CLEANING ===============
    def clean_url(self, url):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø´ÙˆØ§Ø¦Ø¨"""
        if not url:
            return ""
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ string Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ†
        url_str = str(url)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        bad_chars = ['\\', '"', "'", '<', '>', '\n', '\r', '\t', ' ']
        for char in bad_chars:
            url_str = url_str.replace(char, '')
        
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ±Ù…ÙŠØ²
        url_str = url_str.replace('\\/', '/')
        url_str = url_str.replace('\\u002F', '/')
        url_str = url_str.replace('%2F', '/')
        url_str = url_str.replace('%3A', ':')
        
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ø³Ø¨ÙŠØ©
        if url_str.startswith('//'):
            url_str = 'https:' + url_str
        elif url_str.startswith('/'):
            url_str = 'https://www.echoroukonline.com' + url_str
        
        # Ø¥Ø²Ø§Ù„Ø© parameters ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠØ©
        if '|' in url_str:
            url_str = url_str.split('|')[0]
        
        return url_str.strip()
    
    # =============== MAIN EXTRACTION LOGIC ===============
    def extract_all_urls(self):
        """Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ø¬Ù…ÙŠØ¹ Ø·Ø±Ù‚ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬"""
        all_urls = []
        
        Logger.info("=" * 60)
        Logger.info("Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„Ø©")
        Logger.info("=" * 60)
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: yt-dlp (Ø§Ù„Ø£Ù‚ÙˆÙ‰)
        for target_url in CONFIG['target_urls']:
            urls = self.extract_with_ytdlp(target_url)
            all_urls.extend(urls)
            time.sleep(1)  # ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: ØªØ­Ù„ÙŠÙ„ HTML
        for target_url in CONFIG['target_urls'][:2]:  # Ø£ÙˆÙ„ Ø±Ø§Ø¨Ø·ÙŠÙ† ÙÙ‚Ø·
            urls = self.extract_from_html(target_url)
            all_urls.extend(urls)
            time.sleep(1)
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 3: Ø§ÙƒØªØ´Ø§Ù CDN
        cdn_urls = self.discover_cdn_urls()
        all_urls.extend(cdn_urls)
        
        # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 4: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø¨ÙƒØ©
        urls = self.analyze_network_requests(CONFIG['target_urls'][0])
        all_urls.extend(urls)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        unique_urls = []
        seen = set()
        for url in all_urls:
            if url and url not in seen:
                seen.add(url)
                unique_urls.append(url)
        
        self.results['urls_found'] = unique_urls
        Logger.success(f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©: {len(unique_urls)}")
        
        return unique_urls
    
    def validate_all_urls(self, urls):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·"""
        Logger.info(f"Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† {len(urls)} Ø±Ø§Ø¨Ø·...")
        
        working_urls = []
        
        for i, url in enumerate(urls[:20]):  # Ø§Ø®ØªØ¨Ø± Ø£ÙˆÙ„ 20 Ø±Ø§Ø¨Ø· ÙÙ‚Ø·
            Logger.info(f"Ø§Ù„ØªØ­Ù‚Ù‚ {i+1}/{min(20, len(urls))}: {url[:60]}...")
            
            validated_url = self.validate_stream_url(url)
            if validated_url:
                working_urls.append(validated_url)
                Logger.success(f"âœ… ÙŠØ¹Ù…Ù„!")
            
            # ÙˆÙ‚ÙØ© Ù‚ØµÙŠØ±Ø© Ø¨ÙŠÙ† Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
            if (i + 1) % 5 == 0:
                time.sleep(1)
        
        self.results['urls_working'] = working_urls
        Logger.success(f"Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¹Ø§Ù…Ù„Ø©: {len(working_urls)}")
        
        return working_urls
    
    def select_best_url(self, urls):
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø±Ø§Ø¨Ø·"""
        if not urls:
            return None
        
        # Ù†Ø¸Ø§Ù… ØªÙ‚ÙŠÙŠÙ… Ø°ÙƒÙŠ
        def score_url(url):
            score = 0
            url_lower = url.lower()
            
            # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ù…Ù‡Ù…Ø©
            keywords = {
                'echorouk': 20,
                'news': 15,
                'live': 10,
                'stream': 10,
                'hls': 5,
                'v7.vcloud': 25,  # CDN Ù…ÙˆØ«ÙˆÙ‚
                'algeriatv': 20,
                'alaan': 15
            }
            
            for keyword, points in keywords.items():
                if keyword in url_lower:
                    score += points
            
            # HTTPS Ø£ÙØ¶Ù„
            if url.startswith('https://'):
                score += 10
            
            # Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø£ÙØ¶Ù„
            if len(url) < 150:
                score += 5
            
            # Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø¨Ø¯ÙˆÙ† parameters ÙƒØ«ÙŠØ±Ø© Ø£ÙØ¶Ù„
            if '?' not in url:
                score += 3
            
            return score
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø±Ø§Ø¨Ø· Ø¨Ø£Ø¹Ù„Ù‰ ØªÙ‚ÙŠÙŠÙ…
        best_url = max(urls, key=score_url)
        self.results['best_url'] = best_url
        
        Logger.success(f"Ø£ÙØ¶Ù„ Ø±Ø§Ø¨Ø· Ù…Ø®ØªØ§Ø±: {best_url[:80]}...")
        
        return best_url
    
    def create_m3u_file(self, url):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù M3U Ø§Ø­ØªØ±Ø§ÙÙŠ"""
        if not url:
            return None
        
        # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ù€ IPTV
        headers_part = f"|User-Agent={CONFIG['user_agent']}&Referer=https://www.echoroukonline.com/"
        final_url = url + headers_part
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ù†Ø§Ø©
        now = datetime.now()
        m3u_content = f"""#EXTM3U x-tvg-url="http://epg.51zmt.top:8000/e.xml" url-tvg="http://epg.51zmt.top:8000/e.xml"
#EXTINF:-1 tvg-id="EchoroukNews.dz" tvg-name="Ø§Ù„Ø´Ø±ÙˆÙ‚ Ù†ÙŠÙˆØ²" tvg-logo="https://www.echoroukonline.com/images/logo.png" group-title="ğŸ‡©ğŸ‡¿ Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ø¬Ø²Ø§Ø¦Ø±",Ø§Ù„Ø´Ø±ÙˆÙ‚ Ù†ÙŠÙˆØ² - Ø§Ù„Ø¨Ø« Ø§Ù„Ø­ÙŠ
{final_url}

# ğŸ¥ Echorouk News Live Stream
# ğŸ”„ ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¢Ù„ÙŠØ§Ù‹: {now.strftime('%Y-%m-%d %H:%M:%S')}
# ğŸ“¡ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£ØµÙ„ÙŠ: {url}
# âš¡ Ø§Ù„Ø¥ØµØ¯Ø§Ø±: Super Extractor v4.0
# ğŸ“Š Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¬Ø±Ø¨Ø©: {len(self.results['urls_found'])}
# âœ… Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¹Ø§Ù…Ù„Ø©: {len(self.results['urls_working'])}
"""
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
        os.makedirs('results', exist_ok=True)
        m3u_path = 'results/echorouk_news.m3u'
        
        with open(m3u_path, 'w', encoding='utf-8') as f:
            f.write(m3u_content)
        
        self.results['final_m3u'] = m3u_path
        Logger.success(f"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù M3U: {m3u_path}")
        
        return m3u_path
    
    def save_results(self):
        """Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        os.makedirs('results', exist_ok=True)
        os.makedirs('logs', exist_ok=True)
        
        # Ø­Ø§Ù„Ø© Ø§Ù„ØªÙ†ÙÙŠØ°
        self.results['end_time'] = datetime.now().isoformat()
        self.results['status'] = 'success' if self.results['best_url'] else 'failed'
        self.results['execution_time'] = str(
            datetime.fromisoformat(self.results['end_time']) - 
            datetime.fromisoformat(self.results['start_time'])
        )
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©
        with open('results/extraction_results.json', 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ù…Ø¨Ø³Ø·Ø©
        status_data = {
            'status': self.results['status'],
            'best_url': self.results['best_url'],
            'working_urls_count': len(self.results['urls_working']),
            'last_update': datetime.now().isoformat(),
            'version': 'v4.0'
        }
        
        with open('results/extraction_status.json', 'w', encoding='utf-8') as f:
            json.dump(status_data, f, indent=2)
        
        # Ø­ÙØ¸ Ø§Ù„Ø³Ø¬Ù„
        log_file = f"logs/extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(json.dumps(self.results, indent=2, ensure_ascii=False))
        
        Logger.success(f"ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù…Ø¬Ù„Ø¯ results/")
        
        return self.results['status']
    
    def run(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""
        try:
            # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
            all_urls = self.extract_all_urls()
            
            # 2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
            working_urls = self.validate_all_urls(all_urls)
            
            # 3. Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø±Ø§Ø¨Ø·
            best_url = self.select_best_url(working_urls)
            
            if best_url:
                # 4. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù M3U
                self.create_m3u_file(best_url)
                
                # 5. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                status = self.save_results()
                
                Logger.success("=" * 60)
                Logger.success("ğŸ‰ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§ÙƒØªÙ…Ù„Øª Ø¨Ù†Ø¬Ø§Ø­!")
                Logger.success(f"ğŸ“Š Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¹Ø§Ù…Ù„Ø©: {len(working_urls)}")
                Logger.success(f"ğŸ† Ø£ÙØ¶Ù„ Ø±Ø§Ø¨Ø·: {best_url[:80]}...")
                Logger.success("=" * 60)
                
                return True, best_url
            else:
                Logger.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙŠØ¹Ù…Ù„")
                self.save_results()
                return False, None
                
        except Exception as e:
            Logger.error(f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")
            import traceback
            traceback.print_exc()
            return False, None

# =============== MAIN EXECUTION ===============
def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘   ğŸ¥ ECHOROUK SUPER EXTRACTOR v4.0 - GITHUB ACTIONS EDITION  â•‘
    â•‘   ğŸ”„ ØªØ­Ø¯ÙŠØ« Ø¢Ù„ÙŠ Ù„Ø±Ø§Ø¨Ø· Ø¨Ø« Ø§Ù„Ø´Ø±ÙˆÙ‚ Ù†ÙŠÙˆØ²                       â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    extractor = EchoroukSuperExtractor()
    success, best_url = extractor.run()
    
    # Ø¥Ø®Ø±Ø§Ø¬ Ù„Ù„Ù€ GitHub Actions
    if success:
        print(f"::set-output name=status::success")
        print(f"::set-output name=stream_url::{best_url}")
        print(f"::set-output name=timestamp::{datetime.now().isoformat()}")
        sys.exit(0)
    else:
        print(f"::set-output name=status::failed")
        sys.exit(1)

if __name__ == "__main__":
    main()
