#!/usr/bin/env python3
"""
ğŸ”¥ Ù…Ø³ØªØ®Ø±Ø¬ Ø¨Ø« Ø§Ù„Ø´Ø±ÙˆÙ‚ Ù†ÙŠÙˆØ² Ø§Ù„Ø®Ø§Ø±Ù‚ - Ø¥ØµØ¯Ø§Ø± GitHub Actions
ğŸ”„ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±Ø§Ø¨Ø· ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ ÙƒÙ„ 6 Ø³Ø§Ø¹Ø§Øª
"""

import os
import re
import sys
import json
import time
import hashlib
import requests
import yt_dlp
from datetime import datetime
from typing import Optional, Dict, List, Tuple
from urllib.parse import urlparse, parse_qs

class EchoroukSuperExtractor:
    """ÙØ¦Ø© Ø®Ø§Ø±Ù‚Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¨Ù‚ÙˆØ©"""
    
    def __init__(self):
        self.session = requests.Session()
        self.setup_session()
        self.results = {
            'found_urls': [],
            'working_urls': [],
            'best_url': None,
            'timestamp': datetime.now().isoformat()
        }
        
    def setup_session(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø¬Ù„Ø³Ø© HTTP Ù…ØªÙ‚Ø¯Ù…Ø©"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ar,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'Referer': 'https://www.echoroukonline.com/'
        })
        
    def smart_proxy_rotation(self):
        """ØªØ¯ÙˆÙŠØ± ÙˆÙƒÙŠÙ„ Ø°ÙƒÙŠ (ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© proxies Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù‡Ù†Ø§)"""
        proxies = {
            'http': os.getenv('HTTP_PROXY', ''),
            'https': os.getenv('HTTPS_PROXY', '')
        }
        if any(proxies.values()):
            self.session.proxies.update(proxies)
            
    def extract_with_ytdlp(self, url: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… yt-dlp (Ø§Ù„Ø£Ù‚ÙˆÙ‰)"""
        print("ğŸ” Ø§Ø³ØªØ®Ø¯Ø§Ù… yt-dlp Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…...")
        
        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': True,
            'force_generic_extractor': False,
            'user_agent': self.session.headers['User-Agent'],
            'referer': 'https://www.echoroukonline.com/',
        }
        
        found_urls = []
        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=False)
                
                # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙƒÙ„ Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ù…Ù…ÙƒÙ†Ø©
                if 'formats' in info:
                    for fmt in info['formats']:
                        if fmt.get('protocol') in ['m3u8', 'm3u8_native']:
                            video_url = fmt.get('url')
                            if video_url and '.m3u8' in video_url:
                                found_urls.append(video_url)
                                
                # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ URLs Ù…Ø¨Ø§Ø´Ø±Ø©
                if 'url' in info and '.m3u8' in info['url']:
                    found_urls.append(info['url'])
                    
        except Exception as e:
            print(f"âš ï¸ yt-dlp Ø®Ø·Ø£: {e}")
            
        return list(set(found_urls))
    
    def deep_html_analysis(self, html: str) -> List[str]:
        """ØªØ­Ù„ÙŠÙ„ HTML Ø¹Ù…ÙŠÙ‚ Ø¨Ø§Ø­Ø«Ø§Ù‹ Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ù…Ø®ÙÙŠØ©"""
        patterns = [
            # Ø£Ù†Ù…Ø§Ø· JavaScript
            r'(?i)(?:var|let|const)\s+\w+\s*=\s*["\'](https?://[^"\']+\.m3u8[^"\']*)["\']',
            r'(?i)\.setup\({\s*[^}]*file\s*:\s*["\'](https?://[^"\']+\.m3u8)["\']',
            r'(?i)src:\s*["\'](https?://[^"\']+\.m3u8(?:\?[^"\']+)?)["\']',
            
            # Ø£Ù†Ù…Ø§Ø· JSON
            r'"playlist"\s*:\s*\[\s*{[^}]+"file"\s*:\s*"([^"]+\.m3u8[^"]*)"',
            r'"sources"\s*:\s*\[\s*{[^}]+"src"\s*:\s*"([^"]+\.m3u8[^"]*)"',
            
            # Ø£Ù†Ù…Ø§Ø· HTML5 Video
            r'<video[^>]+data-setup=\'[^\']*"file"\s*:\s*"([^"]+\.m3u8)"',
            
            # Ø±ÙˆØ§Ø¨Ø· CDN Ù…Ø®ØµØµØ© Ù„Ù„Ø¬Ø²Ø§Ø¦Ø±
            r'(https?://(?:[^/]+\.)?(?:algeriatv|echorouk|v7\.vcloud|dzcdn)[^/]+/.*?\.m3u8)',
        ]
        
        found = []
        for pattern in patterns:
            try:
                matches = re.findall(pattern, html, re.DOTALL)
                for match in matches:
                    if isinstance(match, tuple):
                        match = match[0]
                    clean_url = self.clean_url(match)
                    if clean_url and clean_url not in found:
                        found.append(clean_url)
            except:
                continue
                
        return found
    
    def clean_url(self, url: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† Ø§Ù„Ø´ÙˆØ§Ø¦Ø¨"""
        if not url or not isinstance(url, str):
            return ""
            
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ØªØ±Ù…ÙŠØ²
        url = url.replace('\\/', '/').replace('\\u002F', '/')
        url = url.replace('\\/', '/').replace('\\/', '/')
        
        # Ø¥Ø²Ø§Ù„Ø© Ø£Ø­Ø±Ù ØºÙŠØ± Ù…Ø±ØºÙˆØ¨Ø©
        bad_chars = ['\\', '"', "'", '<', '>', '\n', '\r', '\t']
        for char in bad_chars:
            url = url.replace(char, '')
            
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠØ¨Ø¯Ø£ Ø¨Ù€ http
        if not url.startswith('http'):
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ø³Ø¨ÙŠØ©
            if url.startswith('//'):
                url = 'https:' + url
            elif '.m3u8' in url:
                # Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø±Ø§Ø¨Ø·Ø§Ù‹ Ø¨Ø¯ÙˆÙ† Ø¨Ø±ÙˆØªÙˆÙƒÙˆÙ„
                url = 'https://' + url.lstrip('/')
                
        return url.strip()
    
    def test_stream_url(self, url: str, timeout: int = 10) -> bool:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠØ¹Ù…Ù„"""
        try:
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù€ headers Ø§Ù„Ù…Ø±ÙÙ‚Ø© Ù„Ù€ IPTV
            clean_url = url.split('|')[0] if '|' in url else url
            
            # Ø·Ù„Ø¨ HEAD Ø£ÙˆÙ„Ø§Ù‹ Ù„Ù„Ø³Ø±Ø¹Ø©
            head_response = self.session.head(
                clean_url, 
                timeout=timeout,
                allow_redirects=True
            )
            
            if head_response.status_code == 200:
                # Ø¥Ø°Ø§ Ù†Ø¬Ø­ HEADØŒ Ø§Ø®ØªØ¨Ø± Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰
                response = self.session.get(
                    clean_url, 
                    timeout=timeout,
                    stream=True,
                    headers={'Range': 'bytes=0-1000'}  # Ø£ÙˆÙ„ 1000 Ø¨Ø§ÙŠØª ÙÙ‚Ø·
                )
                
                if response.status_code in [200, 206]:
                    content = response.text[:500]
                    return '#EXTM3U' in content or '.m3u8' in content.lower()
                    
        except Exception as e:
            print(f"âš ï¸ ÙØ´Ù„ Ø§Ø®ØªØ¨Ø§Ø± {url[:50]}...: {str(e)[:50]}")
            
        return False
    
    def fetch_all_possible_sources(self) -> List[str]:
        """Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©"""
        sources = []
        
        # Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        base_sources = [
            "https://www.echoroukonline.com/live-news",
            "https://www.echoroukonline.com/tv/live",
            "https://www.echoroukonline.com/en/live",
            "https://www.echoroukonline.com/ar/tv",
        ]
        
        # Ø±ÙˆØ§Ø¨Ø· API Ù…Ø­ØªÙ…Ù„Ø©
        api_sources = [
            "https://api.echoroukonline.com/live/stream",
            "https://www.echoroukonline.com/api/v1/stream",
            "https://player.echoroukonline.com/config.json",
        ]
        
        # Ø±ÙˆØ§Ø¨Ø· CDN Ù…Ø­ØªÙ…Ù„Ø© (ØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡Ø§)
        cdn_sources = [
            "https://streaming.echoroukonline.com/live.m3u8",
            "https://cdn.echoroukonline.com/hls/stream.m3u8",
            "https://live.echoroukonline.com/stream/playlist.m3u8",
            "https://tv.echorouk.tv/live/echorouk_news/index.m3u8",
        ]
        
        all_sources = base_sources + api_sources + cdn_sources
        
        for source in all_sources:
            try:
                print(f"ğŸ” ÙØ­Øµ: {source}")
                
                if source.endswith('.m3u8'):
                    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø±Ø§Ù‹ Ù„Ù€ m3u8
                    if self.test_stream_url(source):
                        sources.append(source)
                else:
                    # Ø¬Ù„Ø¨ ÙˆØªØ­Ù„ÙŠÙ„ HTML
                    response = self.session.get(source, timeout=15)
                    
                    # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 1: yt-dlp
                    yt_urls = self.extract_with_ytdlp(source)
                    sources.extend(yt_urls)
                    
                    # Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© 2: ØªØ­Ù„ÙŠÙ„ HTML
                    html_urls = self.deep_html_analysis(response.text)
                    sources.extend(html_urls)
                    
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ {source}: {str(e)[:50]}")
                continue
                
        return list(set(filter(None, sources)))
    
    def select_best_url(self, urls: List[str]) -> Optional[str]:
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø±Ø§Ø¨Ø· ÙŠØ¹Ù…Ù„"""
        working_urls = []
        
        print(f"ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± {len(urls)} Ø±Ø§Ø¨Ø·...")
        
        for i, url in enumerate(urls, 1):
            print(f"  {i}/{len(urls)}: Ø§Ø®ØªØ¨Ø§Ø± {url[:60]}...")
            
            if self.test_stream_url(url):
                working_urls.append(url)
                print(f"    âœ… ÙŠØ¹Ù…Ù„!")
            else:
                print(f"    âŒ Ù„Ø§ ÙŠØ¹Ù…Ù„")
                
            # ÙˆÙ‚ÙØ© Ù‚ØµÙŠØ±Ø© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
            if i % 3 == 0:
                time.sleep(1)
        
        self.results['working_urls'] = working_urls
        
        if not working_urls:
            return None
            
        # Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø±Ø§Ø¨Ø·
        def url_score(test_url: str) -> int:
            score = 0
            url_lower = test_url.lower()
            
            # Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…Ø¹ÙŠÙ†Ø©
            keywords = ['echorouk', 'news', 'live', 'stream', 'hls']
            for keyword in keywords:
                if keyword in url_lower:
                    score += 10
            
            # Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¢Ù…Ù†Ø© (HTTPS)
            if test_url.startswith('https://'):
                score += 5
                
            # Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù‚ØµÙŠØ±Ø© (Ø£Ù‚Ù„ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù„Ù„Ø§Ù†ØªÙ‡Ø§Ø¡)
            if len(test_url) < 150:
                score += 3
                
            return score
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø±Ø§Ø¨Ø· Ø¨Ø£Ø¹Ù„Ù‰ Ø¯Ø±Ø¬Ø©
        best_url = max(working_urls, key=url_score)
        self.results['best_url'] = best_url
        
        return best_url
    
    def format_for_iptv(self, url: str) -> str:
        """ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ù…Ù„ÙØ§Øª IPTV"""
        if not url:
            return ""
            
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù€ headers Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©
        headers = {
            'User-Agent': self.session.headers['User-Agent'],
            'Referer': 'https://www.echoroukonline.com/'
        }
        
        # Ø¨Ù†Ø§Ø¡ Ø³Ø·Ø± Ø§Ù„Ù€ headers
        header_parts = []
        for key, value in headers.items():
            header_parts.append(f'{key}={value}')
        
        return f"{url}|{'&'.join(header_parts)}"
    
    def create_playlist_file(self, stream_url: str) -> bool:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù M3U Ø§Ø­ØªØ±Ø§ÙÙŠ"""
        if not stream_url:
            return False
            
        try:
            # ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø±Ø§Ø¨Ø·
            iptv_url = self.format_for_iptv(stream_url)
            
            # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù‚Ù†Ø§Ø©
            channel_info = {
                'name': 'Ø§Ù„Ø´Ø±ÙˆÙ‚ Ù†ÙŠÙˆØ²',
                'logo': 'https://www.echoroukonline.com/images/logo.png',
                'group': 'Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ø¬Ø²Ø§Ø¦Ø± ğŸ‡©ğŸ‡¿',
                'id': 'EchoroukNews.dz'
            }
            
            # Ù…Ø­ØªÙˆÙ‰ Ù…Ù„Ù M3U
            m3u_content = f"""#EXTM3U x-tvg-url="http://epg.51zmt.top:8000/e.xml.gz"
#EXTINF:-1 tvg-id="{channel_info['id']}" tvg-name="{channel_info['name']}" tvg-logo="{channel_info['logo']}" group-title="{channel_info['group']}",{channel_info['name']} [Ø¢Ù„ÙŠ Ø§Ù„ØªØ­Ø¯ÙŠØ«: {datetime.now().strftime('%Y-%m-%d %H:%M')}]
{iptv_url}

# ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ø¯ÙŠØ«: {datetime.now().isoformat()}
# Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¬Ø±Ø¨Ø©: {len(self.results['found_urls'])}
# Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¹Ø§Ù…Ù„Ø©: {len(self.results['working_urls'])}
# Ø£ÙØ¶Ù„ Ø±Ø§Ø¨Ø·: {self.results['best_url']}
"""
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
            with open('echorouk_news.m3u', 'w', encoding='utf-8') as f:
                f.write(m3u_content)
            
            # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ€ JSON Ù„Ù„ØªØ­Ù„ÙŠÙ„
            with open('extraction_results.json', 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
            
            return True
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ù: {e}")
            return False
    
    def run_extraction(self) -> Tuple[bool, str]:
        """ØªØ´ØºÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""
        print("=" * 70)
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø« Ø§Ù„Ø´Ø±ÙˆÙ‚ Ù†ÙŠÙˆØ² - GitHub Actions Edition")
        print("=" * 70)
        
        # 1. Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        print("\nğŸ“¡ Ù…Ø±Ø­Ù„Ø© 1: Ø¬Ù…Ø¹ Ø§Ù„Ù…ØµØ§Ø¯Ø±...")
        all_urls = self.fetch_all_possible_sources()
        self.results['found_urls'] = all_urls
        
        if not all_urls:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø±ÙˆØ§Ø¨Ø·")
            return False, ""
        
        print(f"âœ… ØªÙ… Ø¬Ù…Ø¹ {len(all_urls)} Ø±Ø§Ø¨Ø· Ù…Ø­ØªÙ…Ù„")
        
        # 2. Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø±Ø§Ø¨Ø·
        print("\nğŸ† Ù…Ø±Ø­Ù„Ø© 2: Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ø±Ø§Ø¨Ø·...")
        best_url = self.select_best_url(all_urls)
        
        if not best_url:
            print("âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø±ÙˆØ§Ø¨Ø· ØªØ¹Ù…Ù„")
            return False, ""
        
        print(f"ğŸ¯ Ø£ÙØ¶Ù„ Ø±Ø§Ø¨Ø·: {best_url}")
        
        # 3. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØªØ´ØºÙŠÙ„
        print("\nğŸ’¾ Ù…Ø±Ø­Ù„Ø© 3: Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØªØ´ØºÙŠÙ„...")
        success = self.create_playlist_file(best_url)
        
        if success:
            print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù echorouk_news.m3u Ø¨Ù†Ø¬Ø§Ø­")
            
            # Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ
            print("\n" + "=" * 70)
            print("ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
            print(f"   â€¢ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¬Ø±Ø¨Ø©: {len(all_urls)}")
            print(f"   â€¢ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¹Ø§Ù…Ù„Ø©: {len(self.results['working_urls'])}")
            print(f"   â€¢ ÙˆÙ‚Øª Ø§Ù„ØªÙ†ÙÙŠØ°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"   â€¢ Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ´ØºÙŠÙ„: {best_url[:80]}...")
            print("=" * 70)
            
            return True, best_url
        else:
            print("âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØªØ´ØºÙŠÙ„")
            return False, ""

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    extractor = EchoroukSuperExtractor()
    success, url = extractor.run_extraction()
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø®Ø±Ø¬ Ù„Ù€ GitHub Actions
    if success:
        print(f"::set-output name=stream_url::{url}")
        print(f"::set-output name=status::success")
        print(f"::set-output name=timestamp::{datetime.now().isoformat()}")
        sys.exit(0)
    else:
        print("::set-output name=status::failed")
        sys.exit(1)

if __name__ == "__main__":
    main()
